# https://cd.lianjia.com/ershoufang/tianfuxinqu/
# https://cd.lianjia.com/ershoufang/tianfuxinqu/pg2/

import re
import urllib.request
from urllib.error import URLError, HTTPError
import csv
from bs4 import BeautifulSoup


# 创建文件，设置追加模式a+，newline保证不空行
# encoding='utf-8'很关键，之前一直报错，
# 'gbk' codec can't encode character '\uf0b7' in position 13: illegal multibyte sequence
# 找了很多方法也没用，这个终于解决了
csvfile = open('F:\data\天府新区普通住宅价格-2017.12.5.csv', 'a+', newline='', encoding='GB18030')
writer = csv.writer(csvfile)

# 构造标题
title = []
title.append(('房源编号', '房屋名称', '单价(元/平方米)', '总价', '其他信息'))
# 写入标题
writer.writerows(title)

url = "http://cd.lianjia.com/ershoufang/tianfuxinqu/"
page = "pg"

# 设置浏览器头信息
user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
headers = {
    'Connection': 'Keep-Alive',
    'Accept': 'text/html, application/xhtml+xml, */*',
    'Accept-Language': 'en-US,en;q=0.8,zh-Hans-CN;q=0.5,zh-Hans;q=0.3',
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko'
}

# # 构造请求
# req = urllib.request.Request(url,headers=headers)
#
# # 抓取网页信息
# response = urllib.request.urlopen(req,timeout=5)
#
# # 将抓取的信息保存
# content1 = response.read().decode('utf-8')
#
# # 获取总页数
# totalPage = re.findall('"totalPage":(.*?),"curPage"',content1,re.S)
# print(totalPage)
j = 1
while j <= 39:
    if j == 1:
        urlV = url+'sf1/'
    else:
        k = str(j)
        urlV = (url+page+k+'sf1/')
    try:
        # 构造请求
        req = urllib.request.Request(urlV, headers=headers)
        # 抓取网页信息
        response = urllib.request.urlopen(req, timeout=5)
        # 将抓取的信息保存
        content = response.read().decode('utf-8')
        # 解析网页
        soup = BeautifulSoup(content, 'lxml')

        # 获取房子id
        houseIdClass = soup.find_all(name='a', attrs={"class": re.compile("img ")})
        # houseId = re.findall('data-houseid="(.*?)"',houseIdClass.string,re.S)
        houseId = []
        for i in houseIdClass:
            temp = i.get('data-housecode')
            houseId.append(temp)

        # 获取房子名称
        houseNameClass = soup.find_all(name='img', attrs={"class": re.compile("lj-lazy")})
        houseName = []
        for i in houseNameClass:
            temp = i.get('alt')
            houseName.append(temp)
            if len(houseName) == len(houseId):
                break

        # 获取房子单价
        priceClass = soup.find_all(name='div', attrs={"class": re.compile("unitPrice")})
        temp1 = []
        for i in priceClass:
            temp = i.get_text()
            temp1.append(temp)
        temp2 = '.'.join(temp1)
        price = re.findall('单价(.*?)元/平米', temp2, re.S)

        # 获取房子总价
        totalPriceClass = soup.find_all(name='div', attrs={"class": re.compile("totalPrice")})
        totalPrice = []
        for i in totalPriceClass:
            temp = i.get_text()
            totalPrice.append(temp)

        # 获取房子其他信息
        houseInfoClass = soup.findAll(name='div', attrs={"class": re.compile("houseInfo")})
        houseInfo = []
        for i in houseInfoClass:
            temp = i.get_text()
            houseInfo.append(temp)

        print('第%d页正在写入' % j)
        data = []
        for i in range(len(houseId)):
            data.append((houseId[i], houseName[i], price[i], totalPrice[i], houseInfo[i]))

        # 写入数据
        writer.writerows(data)
        csvfile.close
    except HTTPError as e:
        print("HTTPError")
    except URLError as e:
        print("URLError")

    j = j + 1

# i = item1[0]
# print(i)
#
# name = re.findall('.jpg" alt="(.*?)"></a>',content,re.S)
# price =  re.findall('data-price="(.*?)"',content,re.S)
# print(price)
# data = []
# for index in range(len(price)):
#     data.append((name[index],price[index]))
#     print(name[index]+' '+'单价：'+price[index]+'元')
# print (len(name))
# title = []
# title.append(('name','price'))
# csvfile = open('lianjia.csv','a+',newline='')
# writer = csv.writer(csvfile)
# writer.writerows(title)
# writer.writerows(data)
# csvfile.close
